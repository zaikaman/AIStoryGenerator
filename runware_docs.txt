Image Inference API
Generate images from text prompts or transform existing ones using Runware's API. Learn how to do image inference for creative and high-quality results.
Introduction
Image inference is a powerful feature that allows you to generate images from text prompts or transform existing images according to your needs. This page is the complete API reference for image inference tasks. All workflows and operations use the single imageInference task type, differentiated through parameter combinations.

Core operations
Text-to-image: Generate images from text descriptions (full guide).
Image-to-image: Transform existing images based on prompts (full guide).
Inpainting: Edit specific areas within images (full guide).
Outpainting: Extend images beyond original boundaries (full guide).
Advanced features
Additional parameters enable specialized capabilities:

Style and control: ControlNet, LoRA, IP-Adapters, Embeddings.
Quality enhancement: Refiners, VAE.
Identity: PuLID, ACE++, PhotoMaker.
Performance and other: Accelerator options, Advanced features.
Each feature includes detailed parameter documentation below.

Request
Our API always accepts an array of objects as input, where each object represents a specific task to be performed. The structure varies depending on the workflow and features used.

The following examples demonstrate how different parameter combinations create specific workflows.

{
  "taskType": "imageInference",
  "taskUUID": "a770f077-f413-47de-9dac-be0b26a35da6",
  "outputType": "URL",
  "outputFormat": "jpg",
  "positivePrompt": "a serene mountain landscape with a crystal-clear lake reflecting the sky",
  "height": 1024,
  "width": 1024,
  "model": "runware:101@1",
  "steps": 30,
  "CFGScale": 7.5,
  "numberResults": 4
}
taskType
string
required
The type of task to be performed. For this task, the value should be imageInference.

taskUUID
string
required
UUID v4
When a task is sent to the API you must include a random UUID v4 string using the taskUUID parameter. This string is used to match the async responses to their corresponding tasks.

If you send multiple tasks at the same time, the taskUUID will help you match the responses to the correct tasks.

The taskUUID must be unique for each task you send to the API.

outputType
"base64Data" | "dataURI" | "URL"
Default: URL
Specifies the output type in which the image is returned. Supported values are: dataURI, URL, and base64Data.

base64Data: The image is returned as a base64-encoded string using the imageBase64Data parameter in the response object.
dataURI: The image is returned as a data URI string using the imageDataURI parameter in the response object.
URL: The image is returned as a URL string using the imageURL parameter in the response object.
outputFormat
"JPG" | "PNG" | "WEBP"
Default: JPG
Specifies the format of the output image. Supported formats are: PNG, JPG and WEBP.

outputQuality
integer
Min: 20
Max: 99
Default: 95
Sets the compression quality of the output image. Higher values preserve more quality but increase file size, lower values reduce file size but decrease quality.

webhookURL
string
Specifies a webhook URL where JSON responses will be sent via HTTP POST when generation tasks complete. For batch requests with multiple results, each completed item triggers a separate webhook call as it becomes available.

Webhooks can be secured using standard authentication methods supported by your endpoint, such as tokens in query parameters or API keys.

// Basic webhook endpoint
https://api.example.com/webhooks/runware
// With authentication token in query
https://api.example.com/webhooks/runware?token=your_auth_token
// With API key parameter
https://api.example.com/webhooks/runware?apiKey=sk_live_abc123
// With custom tracking parameters
https://api.example.com/webhooks/runware?projectId=proj_789&userId=12345
The webhook POST body contains the JSON response for the completed task according to your request configuration.

deliveryMethod
"sync" | "async"
required
Default: sync
Determines how the API delivers task results. Choose between immediate synchronous delivery or polling-based asynchronous delivery depending on your task requirements.

Sync mode ("sync"):

Returns complete results directly in the API response when processing completes within the timeout window. For long-running tasks like video generation or model uploads, the request will timeout before completion, though the task continues processing in the background and results remain accessible through the dashboard.

Async mode ("async"):

Returns an immediate acknowledgment with the task UUID, requiring you to poll for results using getResponse once processing completes. This approach prevents timeout issues and allows your application to handle other operations while waiting.

Polling workflow (async):

Submit request with deliveryMethod: "async".
Receive immediate response with the task UUID.
Poll for completion using getResponse task.
Retrieve final results when status shows "success".
When to use each mode:

Sync: Fast image generation, simple processing tasks.
Async: Video generation, model uploads, or any task that usually takes more than 60 seconds.
Async mode is required for computationally intensive operations to avoid timeout errors.

uploadEndpoint
string
Specifies a URL where the generated content will be automatically uploaded using the HTTP PUT method. The raw binary data of the media file is sent directly as the request body. For secure uploads to cloud storage, use presigned URLs that include temporary authentication credentials.

Common use cases:

Cloud storage: Upload directly to S3 buckets, Google Cloud Storage, or Azure Blob Storage using presigned URLs.
CDN integration: Upload to content delivery networks for immediate distribution.
// S3 presigned URL for secure upload
https://your-bucket.s3.amazonaws.com/generated/content.mp4?X-Amz-Signature=abc123&X-Amz-Expires=3600
// Google Cloud Storage presigned URL
https://storage.googleapis.com/your-bucket/content.jpg?X-Goog-Signature=xyz789
// Custom storage endpoint
https://storage.example.com/uploads/generated-image.jpg
The content data will be sent as the request body to the specified URL when generation is complete.

checkNSFW
boolean
Default: false
This parameter is used to enable or disable the NSFW check. When enabled, the API will check if the image contains NSFW (not safe for work) content. This check is done using a pre-trained model that detects adult content in images.

When the check is enabled, the API will return NSFWContent: true in the response object if the image is flagged as potentially sensitive content. If the image is not flagged, the API will return NSFWContent: false.

If this parameter is not used, the parameter NSFWContent will not be included in the response object.

Adds 0.1 seconds to image inference time and incurs additional costs.

The NSFW filter occasionally returns false positives and very rarely false negatives.

includeCost
boolean
Default: false
If set to true, the cost to perform the task will be included in the response object.

positivePrompt
string
required
A positive prompt is a text instruction to guide the model on generating the image. It is usually a sentence or a paragraph that provides positive guidance for the task. This parameter is essential to shape the desired results.

For example, if the positive prompt is "dragon drinking coffee", the model will generate an image of a dragon drinking coffee. The more detailed the prompt, the more accurate the results.

If you wish to generate an image without any prompt guidance, you can use the special token __BLANK__. This tells the system to generate an image without text-based instructions.

The length of the prompt must be between 2 and 3000 characters.

negativePrompt
string
A negative prompt is a text instruction to guide the model on generating the image. It is usually a sentence or a paragraph that provides negative guidance for the task. This parameter helps to avoid certain undesired results.

For example, if the negative prompt is "red dragon, cup", the model will follow the positive prompt but will avoid generating an image of a red dragon or including a cup. The more detailed the prompt, the more accurate the results.

The length of the prompt must be between 2 and 3000 characters.

seedImage
string
required
When doing image-to-image, inpainting or outpainting, this parameter is required.

Specifies the seed image to be used for the diffusion process. The image can be specified in one of the following formats:

An UUID v4 string of a previously uploaded image or a generated image.
A data URI string representing the image. The data URI must be in the format data:<mediaType>;base64, followed by the base64-encoded image. For example: data:image/png;base64,iVBORw0KGgo....
A base64 encoded image without the data URI prefix. For example: iVBORw0KGgo....
A URL pointing to the image. The image must be accessible publicly.
Supported formats are: PNG, JPG and WEBP.

maskImage
string
required
When doing inpainting, this parameter is required.

Specifies the mask image to be used for the inpainting process. The image can be specified in one of the following formats:

An UUID v4 string of a previously uploaded image or a generated image.
A data URI string representing the image. The data URI must be in the format data:<mediaType>;base64, followed by the base64-encoded image. For example: data:image/png;base64,iVBORw0KGgo....
A base64 encoded image without the data URI prefix. For example: iVBORw0KGgo....
A URL pointing to the image. The image must be accessible publicly.
Supported formats are: PNG, JPG and WEBP.

maskMargin
integer
Min: 32
Max: 128
Adds extra context pixels around the masked region during inpainting. When this parameter is present, the model will zoom into the masked area, considering these additional pixels to create more coherent and well-integrated details.

This parameter is particularly effective when used with masks generated by the Image Masking API, enabling enhanced detail generation while maintaining natural integration with the surrounding image.

strength
float
Min: 0
Max: 1
Default: 0.8
When doing image-to-image or inpainting, this parameter is used to determine the influence of the seedImage image in the generated output. A lower value results in more influence from the original image, while a higher value allows more creative deviation.

referenceImages
string[]
An array containing reference images used to condition the generation process. These images provide visual guidance to help the model generate content that aligns with the style, composition, or characteristics of the reference materials.

This parameter is particularly useful with edit models like FLUX.1 Kontext, where reference images can guide the generation toward specific visual attributes or maintain consistency with existing content. Each image can be specified in one of the following formats:

An UUID v4 string of a previously uploaded image or a generated image.
A data URI string representing the image. The data URI must be in the format data:<mediaType>;base64, followed by the base64-encoded image. For example: data:image/png;base64,iVBORw0KGgo....
A base64 encoded image without the data URI prefix. For example: iVBORw0KGgo....
A URL pointing to the image. The image must be accessible publicly.
Supported formats are: PNG, JPG and WEBP.

Model Architecture	Max Images
FLUX.1 Kontext	2
Ace++	1
Other models	Not supported
outpaint
object
Extends the image boundaries in specified directions. When using outpaint, you must provide the final dimensions using width and height parameters, which should account for the original image size plus the total extension (seedImage dimensions + top + bottom, left + right).

height
integer
required
Min: 128
Max: 2048
Used to define the height dimension of the generated image. Certain models perform better with specific dimensions.

The value must be divisible by 64, eg: 128...512, 576, 640...2048.

width
integer
required
Min: 128
Max: 2048
Used to define the width dimension of the generated image. Certain models perform better with specific dimensions.

The value must be divisible by 64, eg: 128...512, 576, 640...2048.

model
string
required
We make use of the AIR (Artificial Intelligence Resource) system to identify models. This identifier is a unique string that represents a specific model.

You can find the AIR identifier of the model you want to use in our Model Explorer, which is a tool that allows you to search for models based on their characteristics.

vae
string
We make use of the AIR (Artificial Intelligence Resource) system to identify VAE models. This identifier is a unique string that represents a specific model.

The VAE (Variational Autoencoder) can be specified to override the default one included with the base model, which can help improve the quality of generated images.

You can find the AIR identifier of the VAE model you want to use in our Model Explorer, which is a tool that allows you to search for models based on their characteristics.

steps
integer
Min: 1
Max: 100
Default: 20
The number of steps is the number of iterations the model will perform to generate the image. The higher the number of steps, the more detailed the image will be. However, increasing the number of steps will also increase the time it takes to generate the image and may not always result in a better image (some schedulers work differently).

When using your own models you can specify a new default value for the number of steps.

scheduler
string
Default: Model's scheduler
An scheduler is a component that manages the inference process. Different schedulers can be used to achieve different results like more detailed images, faster inference, or more accurate results.

The default scheduler is the one that the model was trained with, but you can choose a different one to get different results.

Schedulers are explained in more detail in the Schedulers page.

seed
integer
Min: 1
Max: 9223372036854776000
Default: Random
A seed is a value used to randomize the image generation. If you want to make images reproducible (generate the same image multiple times), you can use the same seed value.

Note: Random seeds are generated as 32-bit values for platform compatibility, but you can specify any value if your platform supports it (JavaScript safely supports up to 53-bit integers).

CFGScale
float
Min: 0
Max: 50
Default: 7
Guidance scale represents how closely the images will resemble the prompt or how much freedom the AI model has. Higher values are closer to the prompt. Low values may reduce the quality of the results.

clipSkip
integer
Min: 0
Max: 2
Defines additional layer skips during prompt processing in the CLIP model. Some models already skip layers by default, this parameter adds extra skips on top of those. Different values affect how your prompt is interpreted, which can lead to variations in the generated image.

promptWeighting
string
Defines the syntax to be used for prompt weighting.

Prompt weighting allows you to adjust how strongly different parts of your prompt influence the generated image. Choose between compel notation with advanced weighting operations or sdEmbeds for simple emphasis adjustments.

numberResults
integer
Min: 1
Max: 20
Default: 1
Specifies how many images to generate for the given parameters. Each image will have the same parameters but different seeds, resulting in variations of the same concept.

advancedFeatures
object
A container for specialized features that extend the functionality of the image generation process. This object groups advanced capabilities that enhance specific aspects of the generation pipeline.

advancedFeatures
layerDiffuse
layerDiffuse
boolean
Default: false
Enables LayerDiffuse technology, which allows for the direct generation of images with transparency (alpha channels).

When enabled, this feature applies the necessary LoRA and VAE components to produce high-quality transparent images without requiring post-processing background removal.

This is particularly useful for creating product images, overlays, composites, and other content that requires transparency. The output must be in a format that supports transparency, such as PNG.

Note: This feature is only available for the FLUX model architecture. It automatically applies the equivalent of:

  "lora": [{ "model": "runware:120@2" }],
  "vae": "runware:120@4"
acceleratorOptions
object
Advanced caching mechanisms to significantly speed up image generation by reducing redundant computation. This object allows you to enable and configure acceleration technologies for your specific model architecture.

These caching methods will not perform well with stochastic schedulers (those with SDE or Ancestral in the name). The random noise added by these schedulers prevents the cache from working effectively. For best results, use deterministic schedulers like Euler or DDIM.

acceleratorOptions
teaCache
teaCache
boolean
Default: false
Enables or disables the TeaCache feature, which accelerates image generation by reusing past computations.

TeaCache is specifically designed for transformer-based models such as Flux and SD 3, and does not work with UNet models like SDXL or SD 1.5.

This feature is particularly effective for iterative image editing and prompt refinement workflows.

acceleratorOptions
teaCacheDistance
teaCacheDistance
float
Min: 0
Max: 1
Default: 0.5
Controls the aggressiveness of the TeaCache feature. Values range from 0.0 (most conservative) to 1.0 (most aggressive).

Lower values prioritize quality by being more selective about which computations to reuse, while higher values prioritize speed by reusing more computations.

Example: A value of 0.1 is very conservative, maintaining high quality with modest speed improvements, while 0.6 is more aggressive, yielding greater speed gains with potential minor quality trade-offs.

acceleratorOptions
deepCache
deepCache
boolean
Default: false
Enables or disables the DeepCache feature, which speeds up diffusion-based image generation by caching internal feature maps from the neural network.

DeepCache is designed for UNet-based models like SDXL and SD 1.5, and is not applicable to transformer-based models like Flux and SD 3.

DeepCache can provide significant performance improvements for high-throughput scenarios or when generating multiple similar images.

acceleratorOptions
deepCacheInterval
deepCacheInterval
integer
Min: 1
Default: 3
Represents the frequency of feature caching, specified as the number of steps between each cache operation.

A larger interval value will make inference faster but may impact quality. A smaller interval prioritizes quality over speed.

acceleratorOptions
deepCacheBranchId
deepCacheBranchId
integer
Min: 0
Default: 0
Determines which branch of the network (ordered from the shallowest to the deepest layer) is responsible for executing the caching processes.

Lower branch IDs (e.g., 0) result in more aggressive caching for faster generation, while higher branch IDs produce more conservative caching with potentially higher quality results.

puLID
object
PuLID (Pure and Lightning ID Customization) enables fast and high-quality identity customization for text-to-image generation. This object allows you to configure settings for transferring facial characteristics from a reference image to generated images with high fidelity.

acePlusPlus
object
ACE++ is an advanced framework for character-consistent image generation and editing. It supports two distinct workflows: creating new images guided by a reference image, and editing existing images with precise control over specific regions.

Note: When using the acePlusPlus object, you must set the model parameter to runware:102@1 (FLUX Fill).

The referenceImages parameter is required when using ACE++ and must be specified at the root level of the request, outside of the acePlusPlus object.

refiner
object
Refiner models help create higher quality image outputs by incorporating specialized models designed to enhance image details and overall coherence. This can be particularly useful when you need results with superior quality, photorealism, or specific aesthetic refinements. Note that refiner models are only SDXL based.

The refiner parameter is an object that contains properties defining how the refinement process should be configured. You can find the properties of the refiner object below.

embeddings
object[]
Embeddings (or Textual Inversion) can be used to add specific concepts or styles to your generations. Multiple embeddings can be used at the same time.

The embeddings parameter is an array of objects. Each object contains properties that define which embedding model to use. You can find the properties of the embeddings object below.

controlNet
object[]
With ControlNet, you can provide a guide image to help the model generate images that align with the desired structure. This guide image can be generated with our ControlNet preprocessing tool, extracting guidance information from an input image. The guide image can be in the form of an edge map, a pose, a depth estimation or any other type of control image that guides the generation process via the ControlNet model.

Multiple ControlNet models can be used at the same time to provide different types of guidance information to the model.

The controlNet parameter is an array of objects. Each object contains properties that define the configuration for a specific ControlNet model. You can find the properties of the ControlNet object below.

lora
object[]
With LoRA (Low-Rank Adaptation), you can adapt a model to specific styles or features by emphasizing particular aspects of the data. This technique enhances the quality and relevance of the generated images and can be especially useful in scenarios where the generated images need to adhere to a specific artistic style or follow particular guidelines.

Multiple LoRA models can be used at the same time to achieve different adaptation goals.

The lora parameter is an array of objects. Each object contains properties that define the configuration for a specific LoRA model. You can find the properties of the LoRA object below.

ipAdapters
object[]
IP-Adapters enable image-prompted generation, allowing you to use reference images to guide the style and content of your generations. Multiple IP Adapters can be used simultaneously.

The ipAdapters parameter is an array of objects. Each object contains properties that define which IP-Adapter model to use and how it should influence the generation. You can find the properties of the IP-Adapter object below.

providerSettings
object
Contains provider-specific configuration settings that customize the behavior of different AI models and services. Each provider has its own set of parameters that control various aspects of the generation process.

The providerSettings parameter is an object that contains nested objects for each supported provider.

Response
All inference operations return a consistent response format. Results arrive as they complete due to parallel processing.

{
  "data": [
    {
      "taskType": "imageInference",
      "taskUUID": "a770f077-f413-47de-9dac-be0b26a35da6",
      "imageUUID": "77da2d99-a6d3-44d9-b8c0-ae9fb06b6200",
      "imageURL": "https://im.runware.ai/image/ws/0.5/ii/a770f077-f413-47de-9dac-be0b26a35da6.jpg",
      "cost": 0.0013
    }
  ]
}
taskType
string
The API will return the taskType you sent in the request. In this case, it will be imageInference. This helps match the responses to the correct task type.

taskUUID
string
UUID v4
The API will return the taskUUID you sent in the request. This way you can match the responses to the correct request tasks.

imageUUID
string
UUID v4
A unique identifier for the output image. This UUID can be used to reference the image in subsequent operations or for tracking purposes.

The imageUUID is different from the taskUUID. While taskUUID identifies the request, imageUUID identifies the specific image output.

imageURL
string
If outputType is set to URL, this parameter contains the URL of the image to be downloaded.

imageBase64Data
string
If outputType is set to base64Data, this parameter contains the base64-encoded image data.

imageDataURI
string
If outputType is set to dataURI, this parameter contains the data URI of the image.

seed
integer
The seed value that was used to generate this image. This value can be used to reproduce the same image when using identical parameters in another request.

NSFWContent
boolean
If checkNSFW parameter is used, NSFWContent is included informing if the image has been flagged as potentially sensitive content.

true indicates the image has been flagged (is a sensitive image).
false indicates the image has not been flagged.
The filter occasionally returns false positives and very rarely false negatives.

cost
float
if includeCost is set to true, the response will include a cost field for each task object. This field indicates the cost of the request in USD.